{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file manipualtion\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "# balancing dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# visualistaion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# warning supression\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory  creation and file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/GoogleDrive/My Drive/Dev/TrafficAccidents/data/accidents2019.csv\n",
      "/Volumes/GoogleDrive/My Drive/Dev/TrafficAccidents/data/casualties2019.csv\n",
      "/Volumes/GoogleDrive/My Drive/Dev/TrafficAccidents/data/vehicles2019.csv\n"
     ]
    }
   ],
   "source": [
    "# defining the directory to original data\n",
    "cwd = Path('./')\n",
    "root_dir = cwd.resolve().parent\n",
    "\n",
    "original_data_dir = root_dir / 'data'\n",
    "additional_data_dir = root_dir / 'additional_data'\n",
    "\n",
    "# list the .csv files for the project\n",
    "for file in original_data_dir.glob('*.csv'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision_tree': DecisionTreeClassifier(),\n",
       " 'knn': KNeighborsClassifier(),\n",
       " 'logistic_regression': LogisticRegression(),\n",
       " 'bayes': MultinomialNB(),\n",
       " 'rf': RandomForestClassifier()}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_models():\n",
    "    models = {}\n",
    "    models['decision_tree'] = DecisionTreeClassifier()\n",
    "    models['knn'] = KNeighborsClassifier()\n",
    "    models['logistic_regression'] = LogisticRegression()\n",
    "    models['bayes'] = MultinomialNB()\n",
    "    models['rf'] = RandomForestClassifier()\n",
    "    return models\n",
    "\n",
    "models = get_models()\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ym/67nh3yg92pl8g2wbm_vrs8_h0000gn/T/ipykernel_21221/1089652144.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  vehicles = pd.read_csv(original_data_dir / 'vehicles2019.csv')\n",
      "/var/folders/ym/67nh3yg92pl8g2wbm_vrs8_h0000gn/T/ipykernel_21221/1089652144.py:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  casualties = pd.read_csv(original_data_dir / 'casualties2019.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['accident_index', 'longitude', 'latitude', 'police_force',\n",
       "       'accident_severity', 'number_of_vehicles', 'number_of_casualties',\n",
       "       'day_of_week', 'local_authority_(district)',\n",
       "       'local_authority_(highway)', '1st_road_class', '1st_road_number',\n",
       "       'road_type', 'speed_limit', 'junction_detail', 'junction_control',\n",
       "       '2nd_road_class', '2nd_road_number',\n",
       "       'pedestrian_crossing-human_control',\n",
       "       'pedestrian_crossing-physical_facilities', 'light_conditions',\n",
       "       'weather_conditions', 'road_surface_conditions',\n",
       "       'special_conditions_at_site', 'carriageway_hazards',\n",
       "       'urban_or_rural_area', 'did_police_officer_attend_scene_of_accident',\n",
       "       'lsoa_of_accident_location', 'district', 'converted_date',\n",
       "       'converted_time', 'datetime', 'decimal_time', 'day_of_year',\n",
       "       'vehicle_reference_x', 'vehicle_type', 'towing_and_articulation',\n",
       "       'vehicle_manoeuvre', 'vehicle_location-restricted_lane',\n",
       "       'junction_location', 'skidding_and_overturning',\n",
       "       'hit_object_in_carriageway', 'vehicle_leaving_carriageway',\n",
       "       'hit_object_off_carriageway', '1st_point_of_impact',\n",
       "       'was_vehicle_left_hand_drive?', 'journey_purpose_of_driver',\n",
       "       'sex_of_driver', 'age_of_driver', 'age_band_of_driver',\n",
       "       'engine_capacity_(cc)', 'propulsion_code', 'age_of_vehicle',\n",
       "       'driver_imd_decile', 'driver_home_area_type', 'vehicle_imd_decile',\n",
       "       'vehicle_reference_y', 'casualty_reference', 'casualty_class',\n",
       "       'sex_of_casualty', 'age_of_casualty', 'age_band_of_casualty',\n",
       "       'casualty_severity', 'pedestrian_location', 'pedestrian_movement',\n",
       "       'car_passenger', 'bus_or_coach_passenger',\n",
       "       'pedestrian_road_maintenance_worker', 'casualty_type',\n",
       "       'casualty_home_area_type', 'casualty_imd_decile'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cleaned accidents data frame from the pickle file\n",
    "accidents = pd.read_pickle(additional_data_dir / 'accidents_cleaned.pkl')\n",
    "vehicles = pd.read_csv(original_data_dir / 'vehicles2019.csv')\n",
    "casualties = pd.read_csv(original_data_dir / 'casualties2019.csv')\n",
    "\n",
    "# convert column names to lowercase for ease of indexing\n",
    "def lower_columns(df):\n",
    "    \"\"\"\n",
    "    Defintion:\n",
    "        convert column names to lower case\n",
    "    \"\"\"\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "# converting all column names to lower case\n",
    "lower_columns(vehicles)\n",
    "lower_columns(casualties)\n",
    "\n",
    "accidents = pd.merge(accidents, vehicles, on='accident_index')\n",
    "accidents = pd.merge(accidents, casualties, on='accident_index')\n",
    "accidents.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for columns with many values less than 0\n",
    "\n",
    "Any column with more than 10% of values being less than 0 is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['junction_control', '2nd_road_class', 'towing_and_articulation', 'propulsion_code', 'driver_home_area_type', 'casualty_home_area_type']\n"
     ]
    }
   ],
   "source": [
    "# exclude columns with non-numerical data types\n",
    "test = accidents.select_dtypes(exclude=['object', 'datetime64'])\n",
    "\n",
    "ratios = ((test < 0).sum() / test.sum()).to_dict()\n",
    "\n",
    "to_delete = []\n",
    "for k, v in ratios.items():\n",
    "    if v > 0.10:\n",
    "        to_delete.append(k)\n",
    "\n",
    "print(to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually determine data type of features\n",
    "numeric = ['number_of_vehicles', 'number_of_casualties', 'age_of_driver',\n",
    "           'engine_capacity_(cc)', 'age_of_vehicle', 'age_of_casualty']\n",
    "\n",
    "binary = ['was_vehicle_left_hand_drive?', 'sex_of_driver', 'sex_of_casualty']\n",
    "\n",
    "nominal = ['1st_road_class', 'road_type', 'junction_detail', 'light_conditions',\n",
    "           'weather_conditions', 'road_surface_conditions',\n",
    "           'urban_or_rural_area',\n",
    "           'vehicle_type', 'vehicle_manoeuvre',\n",
    "           'junction_location', 'journey_purpose_of_driver', 'casualty_type']\n",
    "\n",
    "ordinal = ['speed_limit', 'day_of_week', 'day_of_year', 'age_band_of_driver', 'age_band_of_casualty']\n",
    "\n",
    "target = 'accident_severity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection:\n",
    "\n",
    "    def __init__(self, df, cat_features, numeric_features, target):\n",
    "        self.df = df\n",
    "        self.cat_features = cat_features\n",
    "        self.numeric_features = numeric_features\n",
    "        self.target = target\n",
    "\n",
    "    def remove_negatives(self):\n",
    "        features = self.numeric_features + self.cat_features\n",
    "        filtered = self.df.filter(features + [self.target], axis=1)\n",
    "        filtered = filtered[(filtered > 0).all(1)]\n",
    "        return filtered\n",
    "\n",
    "    def get_feature_scores(self, dtype='cat', k='all'):\n",
    "        filtered_dataset = self.remove_negatives()\n",
    "        if dtype == 'cat':\n",
    "            features = self.cat_features\n",
    "            selector = SelectKBest(f_classif, k=k)\n",
    "        if dtype == 'num':\n",
    "            features = self.numeric_features\n",
    "            selector = SelectKBest(f_classif, k=k)\n",
    "        selector.fit(filtered_dataset[features], filtered_dataset[self.target])\n",
    "\n",
    "        cols = selector.get_support(indices=True)\n",
    "        p_values = selector.pvalues_\n",
    "\n",
    "        p_values = pd.Series(p_values, dtype='object')\n",
    "        scores = -np.log10(selector.pvalues_)\n",
    "        return cols, scores\n",
    "\n",
    "    def plot_features(self):\n",
    "        cat_features, cat_scores = self.get_feature_scores(dtype='cat', k='all')\n",
    "        num_features, num_scores = self.get_feature_scores(dtype='num', k='all')\n",
    "        features = cat_features + num_features\n",
    "        scores = list(cat_scores) + list(num_scores)\n",
    "        plt.bar(range(len(features)), scores)\n",
    "        plt.xticks(range(len(features)), features, rotation='vertical')\n",
    "        plt.show()\n",
    "    \n",
    "    def get_new_X(self):\n",
    "        filtered_dataset = self.remove_negatives()\n",
    "        cat, _ = self.get_feature_scores(dtype='cat', k=12)\n",
    "        numeric, _ = self.get_feature_scores(dtype='num', k=2)\n",
    "        target = self.target\n",
    "        cols = np.concatenate((cat, numeric))\n",
    "        target_i = filtered_dataset.columns.get_loc(self.target)\n",
    "        cols = np.append(cols, target_i)\n",
    "        new_df = filtered_dataset.iloc[:, cols]\n",
    "        \n",
    "        return new_df\n",
    "\n",
    "fs = FeatureSelection(accidents, nominal+ordinal+binary, numeric, target)\n",
    "\n",
    "filtered = fs.get_new_X()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the model\n",
    "\n",
    "We see that the model is heavily imbalanced towards accident_severity label 3. \n",
    "\n",
    "Hence, SMOTE oversampling should be used to balance the dataset for more accurate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    31739\n",
       "2     7121\n",
       "1      602\n",
       "Name: accident_severity, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.accident_severity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes prior to over-sampling: \n",
      "\n",
      "X_data shape: (39462, 14)\n",
      "y_data shape: (39462,)\n",
      "\n",
      "\n",
      "Shapes after over-sampling: \n",
      "\n",
      "X_data shape: (95217, 14)\n",
      "y_data shape: (95217,)\n"
     ]
    }
   ],
   "source": [
    "X_data = filtered.iloc[:, :-1]\n",
    "y_data = filtered.iloc[:, -1]\n",
    "\n",
    "print(\"Shapes prior to over-sampling: \\n\")\n",
    "print(f\"X_data shape: {X_data.shape}\")\n",
    "print(f\"y_data shape: {y_data.shape}\")\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_data, y_data = oversample.fit_resample(X_data, y_data)\n",
    "\n",
    "print(\"\\n\\nShapes after over-sampling: \\n\")\n",
    "print(f\"X_data shape: {X_data.shape}\")\n",
    "print(f\"y_data shape: {y_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ratio after over-sampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    31739\n",
       "1    31739\n",
       "2    31739\n",
       "Name: accident_severity, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Label ratio after over-sampling\")\n",
    "y_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (71412, 14)\n",
      "y_train shape: (71412,)\n",
      "X_test shape: (23805, 14)\n",
      "y_test shape: (23805,)\n"
     ]
    }
   ],
   "source": [
    "# splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25)\n",
    "\n",
    "# data set shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "\n",
    "Here we use a repeated stratified k-fold cross-validation method, so that the training data is split into k folds with equal proportions per label. Each k-1 folds are evaluated using the final fold as validation, and the process is repeated three times, hence repeated stratified k-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a stacked model and appending to the list of individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('decision_tree', DecisionTreeClassifier()),\n",
       "                               ('knn', KNeighborsClassifier()),\n",
       "                               ('logistic_regression', LogisticRegression()),\n",
       "                               ('bayes', MultinomialNB()),\n",
       "                               ('rf', RandomForestClassifier())],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stacking():\n",
    "    level0 = []\n",
    "    for k, v in models.items():\n",
    "        level0.append((k, v))\n",
    "    level1 = LogisticRegression()\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    "\n",
    "models['stack'] = get_stacking()\n",
    "\n",
    "get_stacking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training all models in the models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision_tree': DecisionTreeClassifier(),\n",
       " 'knn': KNeighborsClassifier(),\n",
       " 'logistic_regression': LogisticRegression(),\n",
       " 'bayes': MultinomialNB(),\n",
       " 'rf': RandomForestClassifier(),\n",
       " 'stack': StackingClassifier(cv=5,\n",
       "                    estimators=[('decision_tree', DecisionTreeClassifier()),\n",
       "                                ('knn', KNeighborsClassifier()),\n",
       "                                ('logistic_regression', LogisticRegression()),\n",
       "                                ('bayes', MultinomialNB()),\n",
       "                                ('rf', RandomForestClassifier())],\n",
       "                    final_estimator=LogisticRegression())}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree 0.8960, 0.0035\n",
      "knn 0.8720, 0.0038\n",
      "logistic_regression 0.4843, 0.0073\n",
      "bayes 0.4019, 0.0075\n",
      "rf 0.9352, 0.0035\n",
      "stack 0.9483, 0.0028\n"
     ]
    }
   ],
   "source": [
    "results, names, summary_results = [], [], []\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    summary_results.append({name: {'mean': np.mean(results), 'std': np.std(results)}})\n",
    "    print(f\"{name} {np.mean(scores):.4f}, {np.std(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'decision_tree': {'mean': 0.8960398812524506, 'std': 0.003458774095393027}},\n",
       " {'knn': {'mean': 0.884020426072182, 'std': 0.012555773525351805}},\n",
       " {'logistic_regression': {'mean': 0.7507701786814541,\n",
       "   'std': 0.18876942419494633}},\n",
       " {'bayes': {'mean': 0.6635579454433428, 'std': 0.2226147857167037}},\n",
       " {'rf': {'mean': 0.7178952183573255, 'std': 0.2268446018975331}},\n",
       " {'stack': {'mean': 0.7562975721478494, 'std': 0.2241809224705417}}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An individual decision tree model has the highest accuracy of all the models tested at 89.6%."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "496d701c09bd200faf5a43ebc3c61dde3e6dfd19068c5afa6d90de8674a96c33"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
