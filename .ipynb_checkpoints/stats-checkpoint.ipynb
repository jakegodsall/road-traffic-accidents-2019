{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a5d797",
   "metadata": {},
   "source": [
    "# Big Data Project\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Loading datasets\n",
    "2. Data cleaning and feature selection\n",
    "3. Exploratory Data Analysis\n",
    "4. Hypothesis I: Football matches\n",
    "5. Hypothesis II:\n",
    "6. Hypothesis III:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0685404",
   "metadata": {},
   "source": [
    "## Packages\n",
    "Importing all necessary packages to run the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# packages for web scraping\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "#packages for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e4867",
   "metadata": {},
   "source": [
    "## Directory navigation and creation\n",
    "Creating pathlib.Path objects for cross-platform navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e1de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Path object for current working directory\n",
    "directory = Path('./')\n",
    "# creating Path object for additional data directory\n",
    "additional_directory = directory / 'additional_data'\n",
    "# create new directory for additional data\n",
    "Path(additional_directory).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048f565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/vehicles2019.csv\n",
      "data/accidents2019.csv\n",
      "data/casualties2019.csv\n"
     ]
    }
   ],
   "source": [
    "# defining the directory to original data\n",
    "directory = Path('./data/')\n",
    "additional_directory = Path('./additional_data')\n",
    "\n",
    "# list the .csv files for the project\n",
    "for file in directory.glob('*.csv'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c4c98",
   "metadata": {},
   "source": [
    "## Reading datasets\n",
    "Reading in the original three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207da135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accident_index</th>\n",
       "      <th>location_easting_osgr</th>\n",
       "      <th>location_northing_osgr</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>police_force</th>\n",
       "      <th>accident_severity</th>\n",
       "      <th>number_of_vehicles</th>\n",
       "      <th>number_of_casualties</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>pedestrian_crossing-human_control</th>\n",
       "      <th>pedestrian_crossing-physical_facilities</th>\n",
       "      <th>light_conditions</th>\n",
       "      <th>weather_conditions</th>\n",
       "      <th>road_surface_conditions</th>\n",
       "      <th>special_conditions_at_site</th>\n",
       "      <th>carriageway_hazards</th>\n",
       "      <th>urban_or_rural_area</th>\n",
       "      <th>did_police_officer_attend_scene_of_accident</th>\n",
       "      <th>lsoa_of_accident_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019010128300</td>\n",
       "      <td>528218.0</td>\n",
       "      <td>180407.0</td>\n",
       "      <td>-0.153842</td>\n",
       "      <td>51.508057</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18/02/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>E01004762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019010152270</td>\n",
       "      <td>530219.0</td>\n",
       "      <td>172463.0</td>\n",
       "      <td>-0.127949</td>\n",
       "      <td>51.436208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15/01/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>E01003117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019010155191</td>\n",
       "      <td>530222.0</td>\n",
       "      <td>182543.0</td>\n",
       "      <td>-0.124193</td>\n",
       "      <td>51.526795</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019010155192</td>\n",
       "      <td>525531.0</td>\n",
       "      <td>184605.0</td>\n",
       "      <td>-0.191044</td>\n",
       "      <td>51.546387</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01000973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019010155194</td>\n",
       "      <td>524920.0</td>\n",
       "      <td>184004.0</td>\n",
       "      <td>-0.200064</td>\n",
       "      <td>51.541121</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01000546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  accident_index  location_easting_osgr  location_northing_osgr  longitude  \\\n",
       "0  2019010128300               528218.0                180407.0  -0.153842   \n",
       "1  2019010152270               530219.0                172463.0  -0.127949   \n",
       "2  2019010155191               530222.0                182543.0  -0.124193   \n",
       "3  2019010155192               525531.0                184605.0  -0.191044   \n",
       "4  2019010155194               524920.0                184004.0  -0.200064   \n",
       "\n",
       "    latitude  police_force  accident_severity  number_of_vehicles  \\\n",
       "0  51.508057             1                  3                   2   \n",
       "1  51.436208             1                  3                   2   \n",
       "2  51.526795             1                  3                   2   \n",
       "3  51.546387             1                  2                   1   \n",
       "4  51.541121             1                  3                   2   \n",
       "\n",
       "   number_of_casualties        date  ...  pedestrian_crossing-human_control  \\\n",
       "0                     3  18/02/2019  ...                                  0   \n",
       "1                     1  15/01/2019  ...                                 -1   \n",
       "2                     1  01/01/2019  ...                                  0   \n",
       "3                     1  01/01/2019  ...                                  0   \n",
       "4                     2  01/01/2019  ...                                  0   \n",
       "\n",
       "  pedestrian_crossing-physical_facilities  light_conditions  \\\n",
       "0                                       5                 1   \n",
       "1                                      -1                 4   \n",
       "2                                       0                 4   \n",
       "3                                       0                 4   \n",
       "4                                       0                 4   \n",
       "\n",
       "  weather_conditions  road_surface_conditions  special_conditions_at_site  \\\n",
       "0                  1                        1                           0   \n",
       "1                  1                        1                           0   \n",
       "2                  1                        1                           0   \n",
       "3                  1                        1                           0   \n",
       "4                  1                        1                           0   \n",
       "\n",
       "   carriageway_hazards  urban_or_rural_area  \\\n",
       "0                    0                    1   \n",
       "1                    0                    1   \n",
       "2                    0                    1   \n",
       "3                    0                    1   \n",
       "4                    0                    1   \n",
       "\n",
       "   did_police_officer_attend_scene_of_accident  lsoa_of_accident_location  \n",
       "0                                            3                  E01004762  \n",
       "1                                            3                  E01003117  \n",
       "2                                            1                  E01000943  \n",
       "3                                            1                  E01000973  \n",
       "4                                            1                  E01000546  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in .csv files to dataframes\n",
    "vehicles = pd.read_csv(directory / 'vehicles2019.csv', dtype={'Accident_Index': str})\n",
    "casualties = pd.read_csv(directory / 'casualties2019.csv', dtype={'Accident_Index': str})\n",
    "accidents = pd.read_csv(directory / 'accidents2019.csv', dtype={'Accident_Index': str,\n",
    "                                                                'LSOA_of_Accident_Location': str})\n",
    "\n",
    "# convert column names to lowercase for ease of indexing\n",
    "def lower_columns(df):\n",
    "    \"\"\"\n",
    "    Defintion:\n",
    "        convert column names to lower case\n",
    "    \"\"\"\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "# converting all column names to lower case\n",
    "lower_columns(vehicles)\n",
    "lower_columns(casualties)\n",
    "lower_columns(accidents)\n",
    "\n",
    "accidents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07326ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accident_index                                    0\n",
       "location_easting_osgr                            28\n",
       "location_northing_osgr                           28\n",
       "longitude                                        28\n",
       "latitude                                         28\n",
       "police_force                                      0\n",
       "accident_severity                                 0\n",
       "number_of_vehicles                                0\n",
       "number_of_casualties                              0\n",
       "date                                              0\n",
       "day_of_week                                       0\n",
       "time                                             63\n",
       "local_authority_(district)                        0\n",
       "local_authority_(highway)                         0\n",
       "1st_road_class                                    0\n",
       "1st_road_number                                   0\n",
       "road_type                                         0\n",
       "speed_limit                                       0\n",
       "junction_detail                                   0\n",
       "junction_control                                  0\n",
       "2nd_road_class                                    0\n",
       "2nd_road_number                                   0\n",
       "pedestrian_crossing-human_control                 0\n",
       "pedestrian_crossing-physical_facilities           0\n",
       "light_conditions                                  0\n",
       "weather_conditions                                0\n",
       "road_surface_conditions                           0\n",
       "special_conditions_at_site                        0\n",
       "carriageway_hazards                               0\n",
       "urban_or_rural_area                               0\n",
       "did_police_officer_attend_scene_of_accident       0\n",
       "lsoa_of_accident_location                      5714\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab621bb",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Creation\n",
    "1. latitude and longitude imputation\n",
    "    \n",
    "    \n",
    "    SOURCE: https://simplemaps.com/data/gb-cities\n",
    "    \n",
    "2. Datetime formatting\n",
    "\n",
    "    SOURCE: https://www.sunrise-and-sunset.com/en/sun/united-kingdom/london/2019/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9a938d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the local_authority.csv file of local_athority data that came with the original accident data\n",
    "local_authorities = pd.read_csv(additional_directory / 'local_authority.csv')\n",
    "local_authorities.columns = ['local_authority_(district)', 'district']\n",
    "\n",
    "# merge the accidents dataframe with the local_authorities dataframe on 'local_authority_(district)'\n",
    "accidents = pd.merge(accidents, local_authorities, on=['local_authority_(district)'])\n",
    "\n",
    "accidents[['longitude', 'latitude', 'local_authority_(district)', 'district']]\n",
    "accidents.district = accidents.district.str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91aaa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude_new</th>\n",
       "      <th>longitude_new</th>\n",
       "      <th>district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.5072</td>\n",
       "      <td>-0.1275</td>\n",
       "      <td>london, city of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.4800</td>\n",
       "      <td>-1.9025</td>\n",
       "      <td>birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.4794</td>\n",
       "      <td>-2.2453</td>\n",
       "      <td>manchester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.7997</td>\n",
       "      <td>-1.5492</td>\n",
       "      <td>leeds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.0077</td>\n",
       "      <td>-1.6578</td>\n",
       "      <td>newcastle upon tyne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>53.4960</td>\n",
       "      <td>-1.4120</td>\n",
       "      <td>rotherham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676</th>\n",
       "      <td>57.2670</td>\n",
       "      <td>-2.1920</td>\n",
       "      <td>aberdeenshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>51.6116</td>\n",
       "      <td>-3.5842</td>\n",
       "      <td>bridgend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>51.1536</td>\n",
       "      <td>1.3714</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>52.0500</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>milton keynes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude_new  longitude_new             district\n",
       "0          51.5072        -0.1275      london, city of\n",
       "1          52.4800        -1.9025           birmingham\n",
       "2          53.4794        -2.2453           manchester\n",
       "3          53.7997        -1.5492                leeds\n",
       "4          55.0077        -1.6578  newcastle upon tyne\n",
       "...            ...            ...                  ...\n",
       "2675       53.4960        -1.4120            rotherham\n",
       "2676       57.2670        -2.1920        aberdeenshire\n",
       "2677       51.6116        -3.5842             bridgend\n",
       "2678       51.1536         1.3714                 kent\n",
       "2679       52.0500        -0.6940        milton keynes\n",
       "\n",
       "[2680 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the latitude, longitude and admin_name columns of the city_coords.csv file\n",
    "cities = pd.read_csv(additional_directory / 'city_coords.csv', usecols=['lat', 'lng', 'admin_name'])\n",
    "cities.columns = ['latitude_new', 'longitude_new', 'district']\n",
    "cities.district = cities.district.str.lower()\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d57d75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Districts with missing coordinate data: {'cheshire east', 'west dorset', 'medway', 'wigan', 'hambleton', 'cheshire west and chester', 'wirral', 'liverpool', 'harrogate', 'scarborough', 'wakefield', 'ryedale', 'powys', 'cardiff', 'selby', 'rugby', 'flintshire', 'warrington', 'adur', 'north east lincolnshire'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hambleton                    4\n",
       "harrogate                    3\n",
       "selby                        3\n",
       "ryedale                      2\n",
       "north east lincolnshire      1\n",
       "cardiff                      1\n",
       "flintshire                   1\n",
       "west dorset                  1\n",
       "adur                         1\n",
       "medway                       1\n",
       "rugby                        1\n",
       "wigan                        1\n",
       "wakefield                    1\n",
       "wirral                       1\n",
       "scarborough                  1\n",
       "warrington                   1\n",
       "cheshire west and chester    1\n",
       "cheshire east                1\n",
       "liverpool                    1\n",
       "powys                        1\n",
       "Name: district, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_districts = accidents[accidents.longitude.isnull()]['district']\n",
    "print(f\"Districts with missing coordinate data: {set(missing_districts)}\")\n",
    "len(missing_districts)\n",
    "missing_districts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb7aecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adur',\n",
       " 'hambleton',\n",
       " 'harrogate',\n",
       " 'rugby',\n",
       " 'ryedale',\n",
       " 'scarborough',\n",
       " 'selby',\n",
       " 'west dorset'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(missing_districts) - set(cities.district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf970804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2688, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_additions = pd.DataFrame(np.array([50.8348, 0.3101, 'adur', \n",
    "                                          54.2959, 1.3135, 'hambleton',\n",
    "                                          53.9921, 1.5418, 'harrogate',\n",
    "                                          52.3709, 1.2650, 'rugby',\n",
    "                                          54.1698, 0.7282, 'ryedale', \n",
    "                                          54.2831, 0.3998, 'scarborough',\n",
    "                                          53.7835, 1.0672, 'selby', \n",
    "                                          50.7755, 2.5817, 'west dorset']).reshape(-1, 3))\n",
    "manual_additions.columns = ['latitude_new', 'longitude_new', 'district']\n",
    "\n",
    "cities = pd.concat([cities, manual_additions], axis=0)\n",
    "\n",
    "cities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7a183c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(missing_districts) - set(cities.district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b372930",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.groupby('district').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c8222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = accidents.merge(cities, on='district')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b44fb57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accident_index                                    0\n",
       "location_easting_osgr                            28\n",
       "location_northing_osgr                           28\n",
       "longitude                                         0\n",
       "latitude                                         28\n",
       "police_force                                      0\n",
       "accident_severity                                 0\n",
       "number_of_vehicles                                0\n",
       "number_of_casualties                              0\n",
       "date                                              0\n",
       "day_of_week                                       0\n",
       "time                                             59\n",
       "local_authority_(district)                        0\n",
       "local_authority_(highway)                         0\n",
       "1st_road_class                                    0\n",
       "1st_road_number                                   0\n",
       "road_type                                         0\n",
       "speed_limit                                       0\n",
       "junction_detail                                   0\n",
       "junction_control                                  0\n",
       "2nd_road_class                                    0\n",
       "2nd_road_number                                   0\n",
       "pedestrian_crossing-human_control                 0\n",
       "pedestrian_crossing-physical_facilities           0\n",
       "light_conditions                                  0\n",
       "weather_conditions                                0\n",
       "road_surface_conditions                           0\n",
       "special_conditions_at_site                        0\n",
       "carriageway_hazards                               0\n",
       "urban_or_rural_area                               0\n",
       "did_police_officer_attend_scene_of_accident       0\n",
       "lsoa_of_accident_location                      5544\n",
       "district                                          0\n",
       "latitude_new                                      0\n",
       "longitude_new                                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e848f36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e17f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.loc[accidents.longitude.isnull(), 'longitude'] = accidents.loc[accidents.longitude.isnull(), 'longitude_new']\n",
    "accidents.loc[accidents.latitude.isnull(), 'latitude'] = accidents.loc[accidents.latitude.isnull(), 'latitude_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71bb2a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accident_index                                    0\n",
       "longitude                                         0\n",
       "latitude                                          0\n",
       "police_force                                      0\n",
       "accident_severity                                 0\n",
       "number_of_vehicles                                0\n",
       "number_of_casualties                              0\n",
       "date                                              0\n",
       "day_of_week                                       0\n",
       "time                                             59\n",
       "local_authority_(district)                        0\n",
       "local_authority_(highway)                         0\n",
       "1st_road_class                                    0\n",
       "1st_road_number                                   0\n",
       "road_type                                         0\n",
       "speed_limit                                       0\n",
       "junction_detail                                   0\n",
       "junction_control                                  0\n",
       "2nd_road_class                                    0\n",
       "2nd_road_number                                   0\n",
       "pedestrian_crossing-human_control                 0\n",
       "pedestrian_crossing-physical_facilities           0\n",
       "light_conditions                                  0\n",
       "weather_conditions                                0\n",
       "road_surface_conditions                           0\n",
       "special_conditions_at_site                        0\n",
       "carriageway_hazards                               0\n",
       "urban_or_rural_area                               0\n",
       "did_police_officer_attend_scene_of_accident       0\n",
       "lsoa_of_accident_location                      5544\n",
       "district                                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents = accidents.drop(['location_easting_osgr', 'location_northing_osgr',\n",
    "                            'latitude_new', 'longitude_new'], axis=1)\n",
    "accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319dc11",
   "metadata": {},
   "source": [
    "### Datetime formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadae9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'converted_date' and 'converted_column' features for manipulation of dates and times\n",
    "accidents['converted_date'] = pd.to_datetime(accidents['date'],\n",
    "                                              format='%d/%m/%Y')\n",
    "accidents['converted_time'] = pd.to_datetime(accidents['time'],\n",
    "                                             errors='coerce',\n",
    "                                             format='%H:%M').dt.time\n",
    "\n",
    "print(f'converted_date dtype: {accidents[\"converted_date\"].dtype}')\n",
    "print(f'converted_time dtype: {accidents[\"converted_time\"].dtype}')\n",
    "print(type(accidents['converted_time'][0]))\n",
    "\n",
    "accidents[['converted_date', 'converted_time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11171f2f",
   "metadata": {},
   "source": [
    "So the converted_date column has a dtype of datetime64[ns]\n",
    "and the converted_time column has a dtype of object consisting of datetime.time elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc675d",
   "metadata": {},
   "source": [
    "### Cleaning the time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a483e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rows with time column == NaT\n",
    "accidents[accidents['converted_time'].isnull()][['converted_date',\n",
    "                                                 'converted_time',\n",
    "                                                'light_conditions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affcb9c",
   "metadata": {},
   "source": [
    "### Checking for correlation between time and light_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "light = accidents.loc[accidents.converted_time.notnull(), ['converted_time', 'light_conditions']]\n",
    "\n",
    "# adding dummy date to converted-time column for manipulation\n",
    "date = str(datetime.datetime.strptime('2018-01-01', '%Y-%m-%d').date())\n",
    "light['converted_time'] = pd.to_datetime(date + \" \" + light.converted_time.astype(str))\n",
    "\n",
    "# do one-hot encoding for the light_conditions column\n",
    "light = pd.concat([light, pd.get_dummies(light.light_conditions)], axis=1).drop(['light_conditions', -1], axis=1)\n",
    "#srss['average_time'] = srss.apply(lambda row: find_avg_time(row.sunrise, row.sunset), axis=1)\n",
    "light.columns = ['time', 'daylight', 'lights_lit',\n",
    "                 'lights_unlit', 'no_lighting', 'unknown']\n",
    "\n",
    "# group the dataframe by hour of day\n",
    "light.groupby(pd.Grouper(key='time', freq='H')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sunrise_sunset.csv as dataframe\n",
    "srss = pd.read_csv(additional_directory / 'sunrise_sunset.csv')\n",
    "\n",
    "# string formatting\n",
    "srss['sunrise'] = srss['sunrise'] + ':00'\n",
    "srss['sunset'] = srss['sunset'] + ':00'\n",
    "srss['day_length'] = srss['day_length'] + ':00'\n",
    "# create converted_date feature as np.datetime64 dtype\n",
    "srss['converted_date'] = pd.to_datetime(srss['date'])\n",
    "# drop original date column\n",
    "srss = srss.drop(['date'], axis=1)\n",
    "# convert sunrise and sunset to np.timedelta64 dtype\n",
    "srss['sunrise'] = pd.to_timedelta(srss['sunrise'])\n",
    "srss['sunset'] = pd.to_timedelta(srss['sunset'])\n",
    "\n",
    "\n",
    "def find_avg_time(first_time, second_time):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Given two times, find the average time between them\n",
    "        (t1 + t2) / 2\n",
    "    \"\"\"\n",
    "    time1_s = first_time.total_seconds()\n",
    "    time2_s = second_time.total_seconds()\n",
    "    time = (time1_s + time2_s) / 2 - 3600\n",
    "    return datetime.datetime.fromtimestamp(time).strftime(\"%H:%M\")\n",
    "    \n",
    "# create average time column using the find_avg_time function\n",
    "srss['average_time'] = srss.apply(lambda row: find_avg_time(row.sunrise, row.sunset), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "srss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns which are not needed\n",
    "srss = srss.drop(['sunrise', 'sunset', 'day_length'], axis=1)\n",
    "# merge useful columns of srss to accidents dataframe on converted_date column\n",
    "accidents = pd.merge(accidents, srss, on=['converted_date'])\n",
    "\n",
    "accidents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7985b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for values with null values in converted_time column\n",
    "missing_time_mask = accidents[\"converted_time\"].isnull()\n",
    "\n",
    "# impute missing times with average time \n",
    "accidents.loc[missing_time_mask, 'converted_time'] = accidents.loc[missing_time_mask, 'average_time']\n",
    "\n",
    "accidents = accidents.drop(['average_time'], axis=1)\n",
    "accidents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single 'datetime' feature in ISO 8601 format\n",
    "# using numpy.datetime64 object\n",
    "\n",
    "accidents['datetime'] = pd.to_datetime(accidents['converted_date'].astype(str) + \" \" + accidents['converted_time'].astype(str),\n",
    "               format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# dropping the original date and time columns\n",
    "accidents = accidents.drop(['date', 'time'], axis=1)\n",
    "accidents['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6381351",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de74127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a decimal time column using the time component of 'converted_time'\n",
    "def convert_to_decimal(df, new_col, original_col='datetime',):\n",
    "    \"\"\"Convert datetime.time value to decimal\"\"\"\n",
    "    df[new_col] = df[original_col].dt.hour + df[original_col].dt.minute / 60\n",
    "    \n",
    "# create decimal_time column\n",
    "convert_to_decimal(accidents, 'decimal_time', 'datetime')\n",
    "\n",
    "accidents.loc[:, ['datetime', 'decimal_time']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61702dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_day_of_year(datetime_val):\n",
    "    \"\"\"Return the day of the year for a given datetime.date value\"\"\"\n",
    "    return datetime_val.dayofyear\n",
    "\n",
    "accidents['day_of_year'] = accidents['datetime'].apply(to_day_of_year)\n",
    "accidents['day_of_year'] = accidents['day_of_year'].fillna(-10)\n",
    "accidents['day_of_year'] = accidents['day_of_year'].astype('int32')\n",
    "\n",
    "# 10 randomly sampled columns\n",
    "accidents.loc[:, ['datetime', 'day_of_year']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbda848",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad99cd2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(data=accidents,\n",
    "             x='decimal_time',\n",
    "             binwidth=1,\n",
    "             kde=True,\n",
    "             stat='density')\n",
    "ax.set(xlabel='Time of day',\n",
    "       ylabel='Number of accidents',\n",
    "       title='Total number of accidents by time of day.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9187956",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Mon', 'Tue', 'Wed',\n",
    "        'Thu', 'Fri', 'Sat',\n",
    "        'Sun']\n",
    "\n",
    "ax = sns.histplot(data=accidents,\n",
    "             x='day_of_week',\n",
    "             binwidth=.2)\n",
    "ax.set(ylabel='Number of accidents',\n",
    "       xlim=(0, 8),\n",
    "       title='Total number of accidents by day of the week.')\n",
    "\n",
    "print(ax.xaxis.get_major_ticks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0427b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.day_of_week.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe23ead",
   "metadata": {},
   "source": [
    "# Hypothesis: \n",
    "## More accidents at football stadiums on days of football matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549b2f",
   "metadata": {},
   "source": [
    "### Test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05998949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_distance(s_lat, s_lng, e_lat, e_lng):\n",
    "    R = 6373.0\n",
    "    \n",
    "    s_lat = s_lat*np.pi/180\n",
    "    s_lng = np.deg2rad(s_lng)\n",
    "    e_lat = np.deg2rad(e_lat)\n",
    "    e_lng = np.deg2rad(e_lng)\n",
    "    \n",
    "    d = np.sin((e_lat - s_lat)/2)**2 + np.cos(s_lat)*np.cos(e_lat) * np.sin((e_lng - s_lng)/2)**2\n",
    "    \n",
    "    return 2 * R * np.arcsin(np.sqrt(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates for Old Trafford football stadium in Manchester\n",
    "manc = [53.457831502, -2.288165514]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981aa2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_manc = accidents\n",
    "\n",
    "# create feature of the distance (in km) of an accident to Old Trafford\n",
    "acc_manc['dist_from_manc'] = sphere_distance(manc[0], manc[1], acc_manc['latitude'], acc_manc['longitude'])\n",
    "\n",
    "# filter for those accidents within 5km radius\n",
    "distance_mask = acc_manc.dist_from_manc < 5\n",
    "acc_manc = acc_manc[distance_mask]\n",
    "# filter for Sunday\n",
    "sunday_mask = acc_manc['day_of_week'] == 1\n",
    "sunday_manc = acc_manc[sunday_mask]\n",
    "\n",
    "# group by day and count number of accidents\n",
    "sundays = sunday_manc.groupby('converted_date')['accident_index'].count()\n",
    "\n",
    "zscores = stats.zscore(sundays)\n",
    "zscores['2019-02-24']\n",
    "\n",
    "sundays.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2eb8b",
   "metadata": {},
   "source": [
    "The value of accidents on the Sunday of the football match is 1.3 standard deviations away from the average. This deserves further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "football = pd.read_csv('additional_data/football_stats.csv')\n",
    "football['datetime'] = pd.to_datetime(football['datetime'])\n",
    "football['converted_date'] = football['datetime'].dt.date\n",
    "\n",
    "acc_i = accidents.copy()\n",
    "football = football.sort_values('day_of_year')\n",
    "acc_i = acc_i.drop('datetime', axis=1)\n",
    "football = football.drop('datetime', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zscores_list = []\n",
    "for i in range(football.shape[0]):\n",
    "    \n",
    "    coordinates = [football.loc[i, 'latitude'], football.loc[i,'longitude']]\n",
    "    football_day = football.loc[i, 'day_of_year']\n",
    "    football_day_of_week = football.loc[i, 'day_of_week']\n",
    "    football_stadium = football.loc[i, 'stadium_name']\n",
    "    \n",
    "    acc_i = accidents.copy()\n",
    "    \n",
    "    # add distance from stadium as a feature\n",
    "    acc_i['dist_from_stadium'] = sphere_distance(coordinates[0], coordinates[1],\n",
    "                                                 acc_i['latitude'], acc_i['longitude'])\n",
    "\n",
    "    # filter for those accidents within 5km radius of the stadium\n",
    "    distance_mask = acc_i['dist_from_stadium'] < 10\n",
    "    # filter for that day of the week\n",
    "    day_of_week_mask = acc_i['day_of_week'] == football_day_of_week\n",
    "\n",
    "    final = acc_i[distance_mask & day_of_week_mask]\n",
    "    \n",
    "\n",
    "    final = final.groupby('day_of_year')['accident_index'].count()\n",
    "    \n",
    "    zscores = stats.zscore(final)\n",
    "    mean = final.mean()\n",
    "    \n",
    "    if football_day in zscores.index:\n",
    "        zscore = zscores[football_day]\n",
    "        accidents_on_day = final[football_day]\n",
    "    else:\n",
    "        zscore = 0\n",
    "        accidents_on_day = 0\n",
    "    \n",
    "    info = {\n",
    "        'Day of match': football_day,\n",
    "        'Stadium': football_stadium,\n",
    "        'Accidents on day of match': accidents_on_day,\n",
    "        'Mean # Accidents in area': mean,\n",
    "        'z_score': zscore\n",
    "    }\n",
    "    \n",
    "        \n",
    "    zscores_list.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(zscores_list)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f636c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172ab49",
   "metadata": {},
   "source": [
    "# Association Pattern Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssociationRule:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def apriori(self,\n",
    "                min_support=0.5,\n",
    "                use_colnames=False,\n",
    "                max_len=None,\n",
    "                verbose=0):\n",
    "        \"\"\"\n",
    "        Uses the mlxtend.frequent_patterns.apriori algorithm\n",
    "        \"\"\"\n",
    "        df = self.df.iloc[:, 1:]\n",
    "        return apriori(df, min_support,\n",
    "                       use_colnames, max_len, verbose)\n",
    "        \n",
    "    def support(self, Y, X):\n",
    "        \"\"\"\n",
    "        Determine support for two items.\n",
    "        Inputs:\n",
    "            X: antecedent\n",
    "            Y: consequent\n",
    "        Returns:\n",
    "            support value\n",
    "        \"\"\"\n",
    "        if X not in self.df.columns:\n",
    "            raise TypeError(\"Invalid antecedent.\")\n",
    "        elif Y not in self.df.columns:\n",
    "            raise TypeError(\"Invalid consequent.\")\n",
    "        else:\n",
    "            freq_XY = self.df.groupby(X)[Y].value_counts()[1][1]\n",
    "            return freq_XY / self.df.shape[0]\n",
    "\n",
    "    def confidence(self, Y, X):\n",
    "        \"\"\"\n",
    "        Determine confidence for two items.\n",
    "        Inputs:\n",
    "            X: antecedent\n",
    "            Y: consequent\n",
    "        Returns:\n",
    "            confidence value\n",
    "        \"\"\"\n",
    "        if X not in self.df.columns:\n",
    "            raise TypeError(\"Invalid antecedent.\")\n",
    "        elif Y not in self.df.columns:\n",
    "            raise TypeError(\"Invalid consequent.\")\n",
    "        else:\n",
    "            freq_X = self.df[X].value_counts()[1] / self.df.shape[0]\n",
    "            return self.support(X, Y) / freq_X\n",
    "        \n",
    "    def lift(self, Y, X):\n",
    "        \"\"\"\n",
    "        Determine the confidence for two items.\n",
    "        Inputs:\n",
    "            X: antecedent\n",
    "            Y: consequent\n",
    "        Returns:\n",
    "            lift value\n",
    "        \"\"\"\n",
    "        if X not in self.df.columns:\n",
    "            raise TypeError(\"Invalid antecenent.\")\n",
    "        elif Y not in self.df.columns:\n",
    "            raise TypeError(\"Invalid consequent.\")\n",
    "        else:\n",
    "            freq_X = self.df[X].value_counts()[1] / self.df.shape[0]\n",
    "            freq_Y = self.df[Y].value_counts()[1] / self.df.shape[0]\n",
    "            return self.support(X, Y) / (freq_X * freq_Y)\n",
    "        \n",
    "        \n",
    "    def report(self, Y, X):\n",
    "        \"\"\"\n",
    "        Prints a short summary report.\n",
    "        Inputs:\n",
    "            X: antecedent\n",
    "            Y: consequent\n",
    "        \"\"\"\n",
    "        if X not in self.df.columns:\n",
    "            raise TypeError(\"Invalid antecedent.\")\n",
    "        elif Y not in self.df.columns:\n",
    "            raise TypeError(\"Invalid consequent.\")\n",
    "        else:\n",
    "            sup = self.support(X, Y)\n",
    "            conf = self.confidence(X, Y)\n",
    "            title = f'{X} -> {Y}'\n",
    "            print(title)\n",
    "            print('-' * len(title))\n",
    "            print(f'Support: {sup:.2f}%')\n",
    "            print(f'Confidence: {conf:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a083c5",
   "metadata": {},
   "source": [
    "How speed limit affects casualty rates\n",
    "\n",
    "$$ \\text{Speed Limit} \\rightarrow \\text{Accident Severity}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "apm_cols = ['accident_severity', 'speed_limit', 'weather_conditions']\n",
    "\n",
    "one_hot = pd.DataFrame()\n",
    "\n",
    "for col in te.columns:\n",
    "    dummy = pd.get_dummies(test.loc[:, col], prefix=col)\n",
    "    one_hot = pd.concat([one_hot, dummy], axis=1)\n",
    "    \n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ddc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = AssociationRule(dummies)\n",
    "\n",
    "min_support = 0.2\n",
    "\n",
    "frequent_itemsets = ar.apriori(min_support=min_support, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(frequent_itemsets,\n",
    "                          metric='lift',\n",
    "                          min_threshold=0.5)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe473e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41723d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_support(d):\n",
    "    \"\"\"\n",
    "    Adaptive support algorithm (Hikmawati et al.)\n",
    "    \"\"\"\n",
    "    n_transactions = d.shape[0]\n",
    "    \n",
    "    freq_items = d.sum(axis=0)\n",
    "    n_items = d.shape[1]\n",
    "    \n",
    "    support = freq_items / n_items\n",
    "    criteria = []\n",
    "    utility = support * criteria\n",
    "    total = utility.sum()\n",
    "    avg_total = total / n_items\n",
    "    min_threshold = avg_total / n_transactions\n",
    "    print(freq_items)\n",
    "    return min_threshold\n",
    "    \n",
    "\n",
    "adaptive_support(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = one_hot.sum(axis=0) / one_hot.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa2198",
   "metadata": {},
   "source": [
    "## Workshop 4: Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d22469",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3690836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60305a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698282f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['weather_conditions', 'speed_limit',\n",
    "              'road_surface_conditions', 'light_conditions']\n",
    "\n",
    "usethis = accidents.dropna()\n",
    "usethis.reset_index(drop=True)\n",
    "\n",
    "usethis = usethis.filter(predictors, axis=1)\n",
    "\n",
    "wc_pos_mask = usethis.weather_conditions > 0\n",
    "sl_pos_mask = usethis.speed_limit > 0\n",
    "rsc_pos_mask = usethis.road_surface_conditions > 0\n",
    "lc_pos_mask = usethis.light_conditions > 0\n",
    "\n",
    "usethisnow = usethis.loc[wc_pos_mask & sl_pos_mask & rsc_pos_mask & lc_pos_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k='all')\n",
    "selector.fit(usethisnow[predictors], usethisnow['accident_severity'])\n",
    "\n",
    "scores = -np.log(selector.pvalues_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb97f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "casualties.accident_index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2953e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e14f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.zscore(accidents.groupby(accidents.day_of_week)['accident_index'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.groupby(accidents.day_of_week)['accident_index'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce16eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
